{
  "id": "mail_thread_recommendation_job",
  "type": "script",
  "maxRows": 1,
  "script": "\nimport org.apache.spark.sql.{Row, DataFrame}\nimport org.slf4j.LoggerFactory\n\ncase class UserId(id: String)\ncase class ItemId(id: String)\ncase class Pref(userId: UserId, itemId: ItemId, weight: Double)\n\ncase class ItemSim(itemId1: ItemId, itemId2: ItemId, weight: Double)\ncase class UnStructSim(itemId1: String, itemId2: String, weight_d: Double)\n/*\nHOWTO use:\n\nin spark-shell:\ncut and paste line 2 (not including the first line starting with \"package\") all the way up to the comments at the bottom\nthen\ncut and paste the lines inside of the comments at the bottom\nwhen it completes, check solr: http://localhost:8983/solr/#/lucidfind_thread_recs_shard1_replica1/query\n\n */\nobject SimpleTwoHopRecommender extends Serializable {\n  val log = LoggerFactory.getLogger(\"SimpleTwoHopRecommender\")\n\n  def itemRecs(userItemMatrix: DataFrame, userIdCol: String, itemIdCol: String, weightCol: String,\n              recsPerItem: Int = 10, outerProductLimit: Int = 100): DataFrame = {\n    val toPref = (row: Row) =>\n      Pref(UserId(row.getAs[String](userIdCol)), ItemId(row.getAs[String](itemIdCol)), row.getAs[Double](weightCol))\n    val prefMatrix = userItemMatrix.rdd.map(toPref)\n    log.warn(s\"using ${prefMatrix.count()} preferences to compute item similarity recs\")\n    val matrixProduct = prefMatrix.groupBy(_.userId.toString.toLowerCase).flatMap { case (userId, prefs) =>\n      val topPrefs = prefs.toList.sortBy(-_.weight).take(outerProductLimit)\n      for {\n        pref1 <- topPrefs\n        pref2 <- topPrefs\n        if !pref1.itemId.id.equalsIgnoreCase(pref2.itemId.id)\n      } yield {\n        ItemSim(pref1.itemId, pref2.itemId, pref1.weight * pref2.weight)\n      }\n    }\n    log.warn(s\"total num outer product of prefs: ${matrixProduct.count()}\")\n    val matrixSumReduced = matrixProduct.groupBy(sim => (sim.itemId1, sim.itemId2)).map { case (_, sims: Iterable[ItemSim]) =>\n      sims.reduce { (s1: ItemSim, s2: ItemSim) => s1.copy(weight = s1.weight + s2.weight) }\n    }\n    log.warn(s\"reduced outer product size: ${matrixSumReduced.count()}\")\n    val recs = matrixSumReduced.groupBy(_.itemId1).mapValues(_.toList.sortBy(-_.weight).take(recsPerItem)).flatMap(_._2)\n    val unStructRecs = recs.map(s => UnStructSim(s.itemId1.id, s.itemId2.id, s.weight))\n    log.warn(s\"total numRecs: ${unStructRecs.count()}\")\n    import userItemMatrix.sqlContext.implicits._\n    unStructRecs.toDF.withColumnRenamed(\"itemId1\", \"rec_for_\" + itemIdCol).withColumnRenamed(\"itemId2\", itemIdCol)\n  }\n}\n val opts = Map(\"zkhost\" -> \"localhost:9983\", \"collection\" -> \"lucidfind_signals_aggr\", \"query\" -> \"*:*\")\n val tmpDF = sqlContext.read.format(\"solr\").options(opts).load\n val recs = SimpleTwoHopRecommender.itemRecs(tmpDF, \"from_email_s\", \"subject_simple_s\", \"weight_d\", 10, 100)\n import sqlContext.implicits._\n import org.apache.spark.sql.functions._\n val finalRecs = recs//.filter(not($\"rec_for_subject_simple_s\".contains(\"review request\"))).filter(not($\"subject_simple_s\".contains(\"review request\")))\n finalRecs.write.format(\"solr\").options(Map(\"zkhost\" -> \"localhost:9983\", \"collection\" -> \"lucidfind_thread_recs\")).mode(org.apache.spark.sql.SaveMode.Overwrite).save\n com.lucidworks.spark.util.SolrSupport.getCachedCloudClient(\"localhost:9983\").commit(\"lucidfind_thread_recs\")\n"
}
